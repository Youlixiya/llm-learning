## 4. 构建一个最小可用的 RAG 知识库问答系统

前面我们已经有了两个重要“积木”：

- 一个能生成文本的开源 Chat 模型（可以选择是否加载 LoRA 适配器）；
- 对 Transformer 和语言模型有了基本直觉。

本章我们来搭建一个 **RAG（Retrieval-Augmented Generation，检索增强生成）** 系统，让模型学会：

> **先查资料，再回答问题。**

---

### 4.1 为什么需要 RAG？

仅靠大模型本身，有几个现实问题很难解决：

- **知识时效性**：预训练数据往往是“历史快照”，无法覆盖最新的业务文档、公司内网资料；
- **领域知识深度**：专业领域（医、法、金融、内部系统文档）很难完全包含在公开预训练语料中；
- **上下文长度限制**：哪怕模型有上万 token 上下文，也不可能每次都把整个知识库塞进去。

RAG 的核心思路是：

1. 把你的知识库（Markdown 文档、接口说明、Wiki 页面等）切成小块并做向量化；
2. 用户提问时，用一个 **文本向量模型（Embedding 模型）** 把问题转成向量；
3. 在向量空间中检索与问题最相似的若干段文档；
4. 把这些文档片段作为“检索上下文”，和用户问题一并输入给大模型，让它在“开卷”的前提下作答。

这样做的好处：

- 不需要重新训练或微调基座模型，就能快速接入你自己的知识；
- 知识变更可以即时生效（重新索引或增量更新就行）；
- 逻辑上更可控，可以追溯“答案依据了哪些原文片段”。

---

### 4.2 本章代码结构与运行方式

本章相关代码位于：

- `src/rag/build_index.py`：从本地文档构建向量索引并持久化到磁盘；
- `src/rag/query_rag.py`：在已有索引上进行检索，并调用 LLM 生成答案。

示例文档放在：

- `data/raw/rag_corpus/`：若干 Markdown 文档，用来模拟技术文档 / 教程内容。

向量索引会写入：

- `data/processed/rag_chroma/`：基于 Chroma 的本地向量库。

#### 4.2.1 构建示例知识库索引

确保依赖安装完成（`chromadb`、`sentence-transformers` 已在 `requirements.txt` 中）：

```bash
python src/rag/build_index.py
```

它会：

1. 扫描 `data/raw/rag_corpus/` 下的所有 `.md` 文件；
2. 使用一个中文友好的向量模型（默认 `BAAI/bge-small-zh-v1.5`）对每个文档做向量化；
3. 将文本、元数据（文件名）和向量写入本地 Chroma 库 `data/processed/rag_chroma/`。

控制台会输出索引到的文档数量。

#### 4.2.2 在知识库上进行 RAG 问答

构建好索引后，就可以在其上进行问答了：

```bash
python src/rag/query_rag.py \
  --question "这套 LLM 教程的整体学习路线是什么？" \
  --top_k 3 \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct
```

脚本会：

1. 使用与索引阶段相同的向量模型，将问题编码为向量；
2. 在 Chroma 中检索最相似的 `top_k` 段文档；
3. 构造一个包含“检索结果 + 用户问题”的提示词，喂给大模型；
4. 输出模型的回答，并打印本次检索到的文档片段来源。

如果你已经用 LoRA 微调过同一个基座模型，也可以在这里把 **微调后的模型作为生成组件** 接入（后续可以扩展支持加载 LoRA 适配器）。

---

### 4.3 代码细节：从文档到向量索引

在 `build_index.py` 中，核心流程大致如下：

1. **读取本地文档**
   - 默认从 `data/raw/rag_corpus/` 读取所有 `.md` 文件；
   - 每个文件作为一份文档，后续可以按需细粒度切分（本示例为简单起见直接使用整篇）。
2. **加载向量模型**
   - 使用 `sentence-transformers` 提供的 `SentenceTransformer`；
   - 示例默认使用 `BAAI/bge-small-zh-v1.5`，兼顾中文效果和资源占用。
3. **编码并写入 Chroma**
   - 对每篇文档得到一个向量表示；
   - 使用 `chromadb.PersistentClient` 创建/获取名为 `"tutorial_rag"` 的集合；
   - 调用 `collection.upsert(...)` 插入文档内容、元数据和向量。

在真实项目中，你可能需要：

- 针对长文档按段落或固定长度切块；
- 存储更多元数据（标签、时间、文档类型等）；
- 支持增量更新和删除（例如文档下线）。

---

### 4.4 代码细节：RAG 查询与答案生成

`query_rag.py` 负责把“检索 + 生成”串起来，核心步骤：

1. **加载 Chroma 索引**
   - 使用与构建阶段相同的 `db_dir` 和集合名；
   - 通过 `collection.query(...)` 进行相似度检索。
2. **构造提示词**
   - 将检索到的若干段文档拼接为“资料列表”，类似：
     ```text
     [资料 1] 来源: 00_intro.md
     ...文档内容...

     [资料 2] 来源: 01_transformer_basics.md
     ...文档内容...
     ```
   - 追加用户问题，并在提示里明确要求模型：
     - 只能根据提供的资料回答；
     - 若资料中没有相关内容，应明确说明“没有找到答案”，不要胡编。
3. **调用 LLM 生成答案**
   - 使用与前一章类似的方式加载 Chat 模型和分词器；
   - 使用 `apply_chat_template` 构造对话；
   - 调用 `generate` 得到最终回答。

这种模式是一种非常通用的工程实践模板：

> **任何需要“基于自有知识回答问题”的场景，本质上都可以看作是一个 RAG 系统。**

---

### 4.5 练习与扩展方向

在掌握基础示例之后，你可以尝试：

1. **替换知识库内容**
   - 把 `data/raw/rag_corpus/` 中的示例文档替换成你自己项目的设计文档 / 接口说明；
   - 观察系统是否能正确回答与这些文档相关的问题。
2. **增加文档切分与重排序**
   - 对长文档按段落或固定长度切分；
   - 对检索结果增加一个基于 LLM 的重排序（re-ranking）步骤，提升相关性；
   - 这些在真实项目中会显著提升答案质量。
3. **接入微调后的模型**
   - 将第 3 章得到的 LoRA 适配器加载到基座模型中；
   - 观察“专用助手 + RAG 知识库”的组合效果。

---

### 4.6 小结

本章我们完成了一个最小可用的 RAG 闭环：

- 从本地 Markdown 文档构建了向量索引；
- 使用向量检索从知识库中找到与用户问题最相关的内容；
- 把检索结果和问题一起交给 LLM，生成有依据的回答。

在后续的 Agent 章节中，我们会让这个 RAG 系统成为 Agent 的一个“工具”，  
从而构建出可以自动决定“什么时候查资料、什么时候直接回答”的多步骤智能体。

