## 0. 引言：为什么还要系统学 LLM？

近年来，大语言模型（LLM）从论文走向大规模应用，但绝大多数开发者只停留在“会调 API”“会写几句 Prompt”。如果想真正把 LLM 用好、用稳、用在生产环境，就需要系统地理解：

- **模型层面**：LLM 本质上在做什么？为什么能“看起来很聪明”？有哪些关键局限？
- **工程层面**：如何在有限算力下微调？如何构建高质量 RAG 系统？如何评测和部署？
- **产品层面**：什么问题适合用 LLM？如何设计交互、控制风险和成本？

本教程的目标，就是帮助你在 **工程实践** 的视角下，构建一整套对 LLM 的理解和实战能力。

---

### 0.1 教程面向谁？

本教程假设你已经具备：

- 基本的 Python 编程能力；
- 对深度学习有大概印象（例如知道“模型”“参数”“训练”“过拟合”这类概念）；
- 不惧命令行、能安装依赖、能阅读英文文档和报错信息。

如果你有以下需求，这套教程会比较适合你：

- 想从“调 SDK”升级到“能改代码、能做微调和 RAG 系统”；
- 想把自己领域的知识（金融、医疗、法律、教育等）做成专业助手；
- 想把 LLM 能力融入现有业务系统或产品中。

---

### 0.2 教程结构与学习路线

本教程按模块组织，每个模块都包含：

- **概念讲解**：用尽量直观的语言解释核心概念；
- **最小可运行示例**：配套的 Python 代码或 Notebook；
- **工程经验**：重点讲“坑”和“实用技巧”，少讲无关紧要的数学细节；
- **思考与练习**：为你后续扩展留出空间。

模块规划概览：

1. **Tiny Transformer 与语言模型基础**：从零实现一个迷你 Transformer，理解“预测下一个 token”的本质。
2. **指令微调与 LoRA/QLoRA**：在开源模型上做参数高效微调，快速打造你的专用助手。
3. **RAG（检索增强生成）**：构建自己的知识库问答系统，解决“模型不知道最新知识”的问题。
4. **Agent 与工具调用**：让模型学会调用搜索、数据库、计算等工具，完成复杂任务。
5. **评测、对齐与安全**：为你的系统设计统一的评测方式，避免一些常见风险。
6. **部署与工程实践**：用 FastAPI 等方式对外提供稳健的 LLM 服务。

你可以线性学习，也可以按需跳转，例如：

- 只想做知识库问答 → 优先看 RAG 模块；
- 已有现成模型 → 直接从微调或部署模块开始。

---

### 0.3 环境与代码约定

- 主要使用 **Python 3.10+**；
- 深度学习框架选用 **PyTorch**；
- 模型与微调相关部分主要依赖 `transformers`、`datasets`、`peft` 等；
- RAG、Agent、评测和部署部分会选择社区中比较成熟、主流的组件。

本教程所有代码尽量做到：

- **可读性** > 性能微调：在教学阶段，更希望你“看得懂”；
- **可抽象、可复用**：在 `src/` 下组织为模块，方便你在真实项目中复用；
- **有清晰入口**：每个模块都提供命令行脚本或 Notebook 演示如何运行。

---

### 0.4 如何使用这套教程？

推荐的使用方式：

1. 按 `README.md` 指引完成环境安装；
2. 从 `docs/01_transformer_basics.md` 和 `docs/02_tiny_lm_training.md` 开始，一边阅读文档一边运行 `src/tiny_lm` 下的代码；
3. 在理解 Tiny LM 的基础上，逐渐迁移到“用现成大模型 + 微调 + RAG”的工程实践；
4. 尝试把你自己的业务需求，映射到其中一个或多个模块上，并基于教程代码进行改造。

在学习过程中，建议你：

- 经常修改代码、调整参数，观察行为变化；
- 尽量把自己的数据“插入进来”做实验，而不仅仅是跑示例数据；
- 把遇到的问题、踩到的坑记录下来，当作自己的“小型 LLM 运维手册”。

