## 1. 从“预测下一个字”理解 Transformer

这一章的目标是：不用太多数学，只用直觉和小例子，帮你建立这样一个心智模型：

> **大语言模型 = 一个非常大的函数，输入一串 token，它输出下一个 token 的概率分布。**

理解这一点，对后面所有内容（微调、RAG、Agent 等）都非常重要。

---

### 1.1 语言模型在做什么？

以一个极简中文例子：

> 输入：“我今天心情很”  
> 语言模型输出的其实是：  
> - “好” 的概率 = 0.6  
> - “差” 的概率 = 0.2  
> - “一般” 的概率 = 0.1  
> - ……（其它所有词的概率）

在工程里，我们一般不会直接看完整概率分布，而是：

- 选取“概率最高的一个”（贪心解码）；
- 或者进行抽样（如 top-k / top-p 采样），以生成更自然、多样的文本。

> 在 `src/tiny_lm/generate.py` 中，你会看到一个最小实现版本的“采样式生成”。

---

### 1.2 从 N-gram 到 Transformer（直觉版）

**N-gram 语言模型** 的思路很简单：

- 只看前面最近的 N 个词来预测下一个词；
- 例如 N=3，只看 “今天 / 心情 / 很” 来预测“好”；
- 通过数频率估计“在这个上下文里，‘好’ 出现的概率有多大”。

它的问题也很明显：

- **上下文窗口太短**，只能看到局部；
- 新组合容易数据稀疏；
- 很难建模长距离依赖。

**Transformer / 自注意力（Self-Attention）** 的关键改进是：

- 对每个位置的 token，它都可以“看到”前面（以及有时后面）的所有 token；
- 通过注意力权重，动态决定“当前 token 应该多看谁、少看谁”；
- 多层堆叠后，可以捕捉更加复杂的模式。

不需要记公式，先只要记住这句话：

> **Self-Attention 就像开会时，每个人可以根据当前议题，选择更关注谁的发言。**

---

### 1.3 Self-Attention 的直观图解

在一个含有 4 个 token 的句子里：

> “我 / 今天 / 心情 / 很”

对其中的“心情”这个 token 来说：

- 它会分别“看向”句子里的每一个 token（包括自己）：
  - 看“我” → 相关性可能中等；
  - 看“今天” → 相关性较高；
  - 看“心情” → 自身信息；
  - 看“很” → 相关性也高；
- 然后计算出一组注意力权重，比如：
  - 对“我”的权重：0.1  
  - 对“今天”的权重：0.3  
  - 对“心情”的权重：0.2  
  - 对“很”的权重：0.4  
- 最终用这组权重，对其他 token 的信息做加权求和，得到“心情”的新表示。

实际实现中，这些“信息”是高维向量，相关性通过向量点积等方式计算。本章不会深入公式，你只需要知道：

- 注意力权重是**数据驱动的**：对不同句子、不同位置会不一样；
- 多层、多头注意力叠加起来，就能学习到丰富的语义和句法关系。

---

### 1.4 位置编码：模型怎么知道顺序？

Self-Attention 看所有 token，但**原始输入是一个“袋子里的向量”**，如果不告诉模型顺序，它是分不清“今天心情很好”和“很好心情今天”的差别的。

为了解决这个问题，我们会把 **位置信息编码** 进 token 向量里：

- 最简单的做法是：为每个位置学习一个可训练的向量（learnable position embedding）；
- 模型输入变成：`token_embedding + position_embedding`。

在 `src/tiny_lm/model.py` 里你会看到一个简化实现：  
使用 `nn.Embedding` 同时为 token 和位置分别提供嵌入。

---

### 1.5 残差连接与层归一化（LayerNorm）

在工程实践中，Transformer 的稳定训练离不开两件事：

- **残差连接（Residual Connection）**：让每一层在变换特征时，都可以“保留一条原路”；  
- **LayerNorm**：对每一层的输出做归一化，避免数值发散或梯度异常。

直觉理解：

- 残差连接就像“在原道路上叠加一些修修补补”，而不是每一层都完全重建信息；
- LayerNorm 像是“每一层都做一次标准化体检”，保证信号在合理的范围内传递。

在 `TinyTransformerLM` 的实现中，你会看到 PyTorch 自带的 `nn.TransformerEncoderLayer` 已经帮我们封装了这些细节，这也是工程上常用的做法。

---

### 1.6 一个最小的 Transformer 语言模型长什么样？

在 `src/tiny_lm/model.py` 中，我们实现了一个简化版的 Transformer 语言模型，大致结构如下：

- 一个 token 嵌入层（`nn.Embedding`）；
- 一个位置嵌入层（`nn.Embedding`）或等价机制；
- 若干层 `nn.TransformerEncoderLayer` 组成的编码器；
- 一个线性层把隐藏状态映射回词表大小，得到对每个 token 的预测概率。

训练时，我们会：

- 输入一段 token 序列；
- 模型输出对每个位置“下一个 token”的预测；
- 用交叉熵损失（Cross Entropy Loss）和真实下一个 token 做比较；
- 通过反向传播更新参数。

这部分的完整训练流程，请见下一章：`docs/02_tiny_lm_training.md`，以及对应代码 `src/tiny_lm/train.py`。

---

### 1.7 本章小结与思考题

本章你需要记住的要点：

- 语言模型的本质任务是：**给定前文，预测下一个 token**；
- Self-Attention 让模型能够动态决定“更关注哪些 token”；
- 位置编码让模型理解“顺序”的概念；
- 残差连接和 LayerNorm 让深层网络更稳定；
- 实际工程中可以直接使用 `nn.TransformerEncoder` 等高层封装。

可以思考的问题：

1. 如果我们把上下文截断为固定长度（例如 128 个 token），在真实应用中会遇到哪些问题？
2. 为什么在生成长文本时，简单的“取最大概率 token”往往会导致内容重复或“机械感很强”？
3. 你能否手动构造一个非常小的数据集，让 Tiny Transformer 在上面“过拟合”，并观察它生成的文本？

